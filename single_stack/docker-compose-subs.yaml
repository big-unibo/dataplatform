version: "3.9"
services:
  namenode1:
    image: apache/hadoop:3.3.6
    ports:
      - 9870:9870
    volumes:
      - namenode1_data:/hadoop/dfs/name
      - namenode1_config:/opt/hadoop/etc/hadoop/
    command:
      - /bin/bash
      - -c
      - |
        sleep 20
        mkdir -p /hadoop/dfs/name
        sudo chown hadoop /hadoop/dfs/name
        if [ ! -d /hadoop/dfs/name ]; then
          echo "Namenode name directory not found: /hadoop/dfs/name"
          exit 2
        fi

        if [ -z "TEST" ]; then
          echo "Cluster name not specified"
          export CLUSTERNAME=test
        fi

        if [ "`ls -A /hadoop/dfs/name`" == "" ]; then
          echo "Formatting namenode name directory: /hadoop/dfs/name"
          /opt/hadoop/bin/hdfs --config /opt/hadoop/etc/hadoop namenode -format TEST
        else
          echo "/hadoop/dfs/name exists!"
        fi
        hdfs --config /opt/hadoop/etc/hadoop namenode
    deploy:
      placement:
        constraints:
          - node.role == manager
    networks:
      - hadoop-network 
    
  namenode2:
    image: apache/hadoop:3.3.6
    ports:
      - 9871:9871
    volumes:
      - namenode2_data:/hadoop/dfs/name
      - namenode2_config:/opt/hadoop/etc/hadoop/
    command:
      - /bin/bash
      - -c
      - |
        sleep 30
        mkdir -p /hadoop/dfs/name
        sudo chown hadoop /hadoop/dfs/name
        if [ "`ls -A /hadoop/dfs/name`" == "" ]; then
          echo "Bootstrappppppping namenode2"
          /opt/hadoop/bin/hdfs namenode -bootstrapStandby
        fi
        hdfs --config /opt/hadoop/etc/hadoop namenode
        hdfs haadmin -transitionToActive nn1
    deploy:
      placement:
        constraints:
          - node.role == manager
    networks:
      - hadoop-network 

  journal1:
    image: apache/hadoop:3.3.6
    volumes:
      - journal1_data:/data/journalnode
      - hadoop_config:/opt/hadoop/etc/hadoop/
    command: 
      - /bin/bash
      - -c
      - |
        mkdir -p /data/journalnode
        sudo chown hadoop /data/journalnode
        hdfs journalnode
    deploy:
      placement:
        constraints:
          - node.role == manager
    networks:
      - hadoop-network 

  journal2:
    image: apache/hadoop:3.3.6
    volumes:
      - journal2_data:/data/journalnode
      - hadoop_config:/opt/hadoop/etc/hadoop/
    command:       
      - /bin/bash
      - -c
      - |
        mkdir -p /data/journalnode
        sudo chown hadoop /data/journalnode
        hdfs journalnode
    deploy:
      placement:
        constraints:
          - node.role == manager
    networks:
      - hadoop-network 

  journal3:
    image: apache/hadoop:3.3.6
    volumes:
      - journal3_data:/data/journalnode
      - hadoop_config:/opt/hadoop/etc/hadoop/
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p /data/journalnode
        sudo chown hadoop /data/journalnode
        hdfs journalnode
    deploy:
      placement:
        constraints:
          - node.role == manager
    networks:
      - hadoop-network 

  datanode:
    image: apache/hadoop:3.3.6
    volumes:
      - datanode_data:/hadoop/dfs/data
      - hadoop_config:/opt/hadoop/etc/hadoop/
    command:
      - /bin/bash
      - -c
      - |
        sleep 30
        sudo chown hadoop /hadoop/dfs/data
        if [ ! -d /hadoop/dfs/data ]; then
          echo "Datanode data directory not found: /hadoop/dfs/data"
          exit 2
        fi
        hdfs --config /opt/hadoop/etc/hadoop datanode
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == worker
    networks:
      - hadoop-network 

  resourcemanager:
    image: apache/hadoop:3.3.6
    ports:
      - 8088:8088
    volumes:
      - hadoop_config:/opt/hadoop/etc/hadoop/
    command: 
      - /bin/bash
      - -c
      - |
        /opt/hadoop/bin/yarn --config /opt/hadoop/etc/hadoop resourcemanager
    deploy:
      placement:
        constraints:
          - node.role == manager
    networks:
      - hadoop-network 

  nodemanager:
    image: apache/hadoop:3.3.6
    volumes:
      - hadoop_config:/opt/hadoop/etc/hadoop/
    command:
      - /bin/bash
      - -c
      - |
        /opt/hadoop/bin/yarn --config /opt/hadoop/etc/hadoop nodemanager
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == worker
    networks:
      - hadoop-network 

  historyserver:
    image: apache/hadoop:3.3.6
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
      - hadoop_config:/opt/hadoop/etc/hadoop/
    ports:
      - 19888:19888
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p /hadoop/yarn/timeline
        sudo chown hadoop /hadoop/yarn/timeline
        /opt/hadoop/bin/yarn --config /opt/hadoop/etc/hadoop historyserver
    deploy:
      placement:
        constraints:
          - node.role == worker
    networks:
      - hadoop-network 

  spark-master:
    image: apache/spark:latest
    environment:
      - SPARK_NO_DAEMONIZE=true
    volumes:
      - spark_config:/opt/spark/conf/
      - hadoop_config:/opt/hadoop/etc/hadoop/
    ports:
      - 7077:7077
    command:
      - /bin/bash
      - -c
      - |
        cp -r /opt/hadoop/etc/hadoop/* /opt/spark/conf/
        /opt/spark/sbin/start-master.sh
    deploy:
      placement:
        constraints:
          - node.role == manager
    networks:
      - hadoop-network 

  spark-worker:
    image: apache/spark:latest
    environment:
      - SPARK_NO_DAEMONIZE=true
    volumes:
      - spark_config:/opt/spark/conf/
      - hadoop_config:/opt/hadoop/etc/hadoop/
    command:
      - /bin/bash
      - -c
      - |
        cp -r /opt/hadoop/etc/hadoop/* /opt/spark/conf/
        /opt/spark/sbin/start-worker.sh spark://spark-master:7077
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == worker
    networks:
      - hadoop-network 

  spark-history-server:
    image: apache/spark:latest
    environment:
      - SPARK_NO_DAEMONIZE=true
    volumes:
      - spark_config:/opt/spark/conf/
      - hadoop_config:/opt/hadoop/etc/hadoop/
    ports:
      - 18080:18080
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p tmp/spark-events/
        sudo chown spark tmp/spark-events/
        if [ ! -d tmp/spark-events/ ]; then
          echo "Spark logs data directory not found"
          exit 2
        fi
        cp -r /opt/hadoop/etc/hadoop/* /opt/spark/conf/
        /opt/spark/sbin/start-history-server.sh
    deploy:
      placement:
        constraints:
          - node.role == worker
    networks:
      - hadoop-network 

  # spark-yarn-test-env:
  #   image: apache/spark:latest
  #   #command: "bash"
  #   environment:
  #     - SPARK_NO_DAEMONIZE=true
  #     - HADOOP_CONF_DIR=/opt/spark/conf/
  #   #env_file:
  #     #- ./dataplatform/conf_files/.env
  #     #- ./dataplatform/conf_files/hadoop.conf
  #   volumes:
  #     - spark_config:/opt/spark/conf/
  #     - hadoop_config:/opt/hadoop/etc/hadoop/
  #     - test_files:/opt/spark/tests
  #   stdin_open: true 
  #   tty: true
  #   command:
  #     - /bin/bash
  #     - -c
  #     - |
  #       cp -r /opt/hadoop/etc/hadoop/* /opt/spark/conf/
  #   deploy:
  #     placement:
  #       constraints:
  #         - node.role == worker
  #   networks:
  #     - hadoop-network 

  portainer:
    init: true
    image: portainer/portainer-ce:latest
    ports:
      - 9443:9443
    volumes:
        - /var/run/docker.sock:/var/run/docker.sock
        - ./portainer_data:/data
    deploy:
      placement:
        constraints:
          - node.role == manager  
    networks:
      - hadoop-network

  zoo1:
    image: zookeeper:latest
    ports:
      - 2181:2181
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=0.0.0.0:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181
    networks:
      - hadoop-network

  zoo2:
    image: zookeeper:latest
    ports:
      - 2182:2181
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=0.0.0.0:2888:3888;2181 server.3=zoo3:2888:3888;2181
    networks:
      - hadoop-network

  zoo3:
    image: zookeeper:latest
    ports:
      - 2183:2181
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=0.0.0.0:2888:3888;2181
    networks:
      - hadoop-network

  kafka:
    image: docker.io/bitnami/kafka:latest
    ports:
      - "9092:9092"
    volumes:
      - kafka_data:/bitnami/
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zoo1:2181,zoo2:2181,zoo3:2181
    deploy:
      placement:
        constraints:
          - node.role == manager 
    networks:
      - hadoop-network

networks:
  hadoop-network:
    driver: overlay

volumes:
  namenode1_config:
    driver_opts:
      type: nfs
      o: addr=137.204.74.55,rw,nfsvers=4
      device: :/mnt/mysharednfs/dataplatform/namenode1_conf/
    
  namenode1_data:
    driver_opts:
      type: nfs
      o: addr=137.204.74.55,rw,nfsvers=4
      device: :/mnt/mysharednfs/dataplatform/namenode1_data/

  namenode2_config:
    driver_opts:
      type: nfs
      o: addr=137.204.74.55,rw,nfsvers=4
      device: :/mnt/mysharednfs/dataplatform/namenode2_conf/

  namenode2_data:
    driver_opts:
      type: nfs
      o: addr=137.204.74.55,rw,nfsvers=4
      device: :/mnt/mysharednfs/dataplatform/namenode2_data/

  journal1_data:
    driver_opts:
      type: nfs
      o: addr=137.204.74.55,rw,nfsvers=4
      device: :/mnt/mysharednfs/dataplatform/journal1_data/

  journal2_data:
    driver_opts:
      type: nfs
      o: addr=137.204.74.55,rw,nfsvers=4
      device: :/mnt/mysharednfs/dataplatform/journal2_data/

  journal3_data:
    driver_opts:
      type: nfs
      o: addr=137.204.74.55,rw,nfsvers=4
      device: :/mnt/mysharednfs/dataplatform/journal3_data/

  datanode_data:
    driver_opts:
      type: nfs
      o: addr=137.204.74.55,rw,nfsvers=4
      device: :/mnt/mysharednfs/dataplatform/datanode_data/

  hadoop_historyserver:
    driver_opts:
      type: nfs
      o: addr=137.204.74.55,rw,nfsvers=4
      device: :/mnt/mysharednfs/dataplatform/yarn_hist_server_data/

  kafka_data:
    driver_opts:
      type: nfs
      o: addr=137.204.74.55,rw,nfsvers=4
      device: :/mnt/mysharednfs/dataplatform/kafka_data/

  spark_config:
    driver_opts:
      type: nfs
      o: addr=137.204.74.55,rw,nfsvers=4
      device: ":/mnt/mysharednfs/dataplatform/spark_conf/"
        
  hadoop_config:
    driver: local
    driver_opts:
      type: nfs
      o: addr=137.204.74.55,rw,nfsvers=4,nolock,hard
      device: ":/mnt/mysharednfs/dataplatform/hadoop_conf/"

#   test_files:
#     driver_opts:
#       type: nfs
#       o: addr=137.204.74.55,rw,nfsvers=4
#       device: :/mnt/mysharednfs/dataplatform/tests/

  