version: "3"

services:
  namenode:
    image: apache/hadoop:${HADOOPVERSION}
    container_name: namenode
    restart: always
    ports:
      - ${NAMENODEPORT}:${NAMENODEPORT}
      - ${NAMENODERPCPORT}:${NAMENODERPCPORT}
    volumes:
      - hadoop_namenode:${NAMEDIR}
    env_file:
      - ./hadoop.conf
      - ./.env
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p /hadoop/dfs/name
        sudo chown hadoop /hadoop/dfs/name
        if [ ! -d $NAMEDIR ]; then
          echo "Namenode name directory not found: $NAMEDIR"
          exit 2
        fi

        if [ -z "$CLUSTERNAME" ]; then
          echo "Cluster name not specified"
          export CLUSTERNAME=test
          echo $CLUSTERNAME CLUSTERNAME
        fi

        if [ "`ls -A $NAMEDIR`" == "" ]; then
          echo "Formatting namenode name directory: $NAMEDIR"
          /opt/hadoop/bin/hdfs --config $HADOOPCONFDIR namenode -format $CLUSTERNAME
        else
          echo "$NAMEDIR exists!"
        fi
        hdfs --config $HADOOPCONFDIR namenode

  datanode:
    image: apache/hadoop:${HADOOPVERSION}
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:${DATADIR}
    env_file:
      - ./hadoop.conf
      - ./.env
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p ${DATADIR}
        sudo chown hadoop ${DATADIR}
        if [ ! -d $DATADIR ]; then
          echo "Datanode data directory not found: $DATADIR"
          exit 2
        fi
        hdfs --config $HADOOPCONFDIR datanode
  
  resourcemanager:
    image: apache/hadoop:${HADOOPVERSION}
    container_name: resourcemanager
    restart: always
    env_file:
      - ./hadoop.conf
      - ./.env
    ports:
      - 8088:8088
    command: 
      - /bin/bash
      - -c
      - |
        /opt/hadoop/bin/yarn --config /opt/hadoop/etc/hadoop resourcemanager

  nodemanager:
    image: apache/hadoop:${HADOOPVERSION}
    container_name: nodemanager
    restart: always
    env_file:
      - ./hadoop.conf
      - ./.env
    command:
      - /bin/bash
      - -c
      - |
        /opt/hadoop/bin/yarn --config /opt/hadoop/etc/hadoop nodemanager

  historyserver:
    image: apache/hadoop:${HADOOPVERSION}
    container_name: historyserver
    restart: always
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.conf
      - ./.env
    ports:
      - ${YARNHISTSERVERPORT}:${YARNHISTSERVERPORT}
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p /hadoop/yarn/timeline
        sudo chown hadoop /hadoop/yarn/timeline
        /opt/hadoop/bin/yarn --config $HADOOPCONFDIR historyserver

  spark-master:
    image: apache/spark:${SPARKVERSION}
    container_name: spark-master
    command: 
       - "/opt/spark/sbin/start-master.sh"
    environment:
      - SPARK_NO_DAEMONIZE=true
    env_file:
      - ./.env
      - ./hadoop.conf
    volumes:
      - ./dataplatform/conf_files/spark_conf/:${SPARKCONFDIR}/
    ports:
      - ${SPARKMASTERPORT}:${SPARKMASTERPORT}
    depends_on:
      - namenode

  spark-worker:
    image: apache/spark:${SPARKVERSION}
    container_name: spark-worker
    command: "/opt/spark/sbin/start-worker.sh spark://${SPARKMASTERHOST}:${SPARKMASTERPORT}"
    environment:
      - SPARK_NO_DAEMONIZE=true
    env_file:
      - ./.env
      - ./hadoop.conf
    volumes:
      - ./dataplatform/conf_files/spark_conf/:${SPARKCONFDIR}/
    restart: always
    depends_on:
      - spark-master

  spark-history-server:
    image: apache/spark:${SPARKVERSION}
    container_name: spark-history-server
    environment:
      - SPARK_NO_DAEMONIZE=true
    env_file:
      - ./.env
      - ./hadoop.conf
    volumes:
      - ./dataplatform/conf_files/spark_conf/:${SPARKCONFDIR}/
    ports:
      - ${SPARKHISTSERVERPORT}:${SPARKHISTSERVERPORT}
    restart: always
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p tmp/spark-events/
        sudo chown spark tmp/spark-events/
        if [ ! -d tmp/spark-events/ ]; then
          echo "Spark logs data directory not found"
          exit 2
        fi
        /opt/spark/sbin/start-history-server.sh
    depends_on:
      - spark-master
      - namenode

  spark-yarn-test-env:
    image: apache/spark:${SPARKVERSION}
    container_name: spark-yarn-test-env
    command: "bash"
    #    >
    #    bash -c "
    #    pip install -r ../tests/requirements.txt &&
    #    export PATH=$PATH:/opt/spark/bin &&
    #    tail -f /dev/null"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - HADOOPCONFDIR=/opt/spark/conf
    env_file:
      - ./.env
      - ./hadoop.conf
    volumes:
      - ./dataplatform/conf_files/spark_conf/:${SPARKCONFDIR}/
      - ./dataplatform/conf_files/hadoop_conf/core-site.xml:${SPARKCONFDIR}/core-site.xml
      - ./dataplatform/conf_files/hadoop_conf/hdfs-site.xml:${SPARKCONFDIR}/hdfs-site.xml
      - ./dataplatform/conf_files/hadoop_conf/yarn-site.xml:${SPARKCONFDIR}/yarn-site.xml
      - ./dataplatform/tests:/opt/spark/tests
    stdin_open: true 
    tty: true 

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
