version: "3"

services:
  namenode1:
    image: apache/hadoop:${HADOOPVERSION}
    container_name: namenode1
    restart: always
    ports:
      - ${NAMENODEPORT}:${NAMENODEPORT}
    volumes:
      - hadoop_namenode1:${NAMEDIR}
    env_file:
      - ./hadoop.conf
      - ./.env
    command:
      - /bin/bash
      - -c
      - |
        sleep 20
        mkdir -p /hadoop/dfs/name
        sudo chown hadoop /hadoop/dfs/name
        if [ ! -d $NAMEDIR ]; then
          echo "Namenode name directory not found: $NAMEDIR"
          exit 2
        fi

        if [ -z "$CLUSTERNAME" ]; then
          echo "Cluster name not specified"
          export CLUSTERNAME=test
        fi

        if [ "`ls -A $NAMEDIR`" == "" ]; then
          echo "Formatting namenode name directory: $NAMEDIR"
          /opt/hadoop/bin/hdfs --config $HADOOPCONFDIR namenode -format $CLUSTERNAME
        else
          echo "$NAMEDIR exists!"
        fi
        hdfs --config $HADOOPCONFDIR namenode
    depends_on:
      - journal1
      - journal2
      - journal3
    
  namenode2:
    image: apache/hadoop:${HADOOPVERSION}
    container_name: namenode2
    restart: always
    ports:
      - ${NAMENODE2PORT}:9871
    volumes:
      - hadoop_namenode2:${NAMEDIR}
    env_file:
      - ./hadoop.conf
      - ./.env
    command:
      - /bin/bash
      - -c
      - |
        sleep 20 &&
        mkdir -p $NAMEDIR
        sudo chown hadoop $NAMEDIR
        if [ "`ls -A $NAMEDIR`" == "" ]; then
          echo "Bootstrappppppping namenode2"
          /opt/hadoop/bin/hdfs namenode -bootstrapStandby
        fi
        hdfs --config $HADOOPCONFDIR namenode
    depends_on:
      - namenode1

  journal1:
    image: apache/hadoop:${HADOOPVERSION}
    hostname: journal1
    env_file:
      - ./hadoop.conf
      - ./.env
    volumes:
      - hadoop_journal1:$JOURNALDIR
    command: 
      - /bin/bash
      - -c
      - |
        mkdir -p $JOURNALDIR
        sudo chown hadoop $JOURNALDIR
        hdfs journalnode

  journal2:
    image: apache/hadoop:${HADOOPVERSION}
    hostname: journal2
    env_file:
      - ./hadoop.conf
      - ./.env
    volumes:
      - hadoop_journal2:$JOURNALDIR
    command:       
      - /bin/bash
      - -c
      - |
        mkdir -p $JOURNALDIR
        sudo chown hadoop $JOURNALDIR
        hdfs journalnode

  journal3:
    image: apache/hadoop:${HADOOPVERSION}
    hostname: journal3
    env_file:
      - ./hadoop.conf
      - ./.env
    volumes:
      - hadoop_journal3:$JOURNALDIR
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p $JOURNALDIR
        sudo chown hadoop $JOURNALDIR
        hdfs journalnode

  activator:
    image: apache/hadoop:${HADOOPVERSION}
    env_file:
      - ./hadoop.conf
      - ./.env
    command:
      - /bin/bash
      - -c
      - |
        sleep 30
        hdfs haadmin -transitionToActive nn1
    restart: on-failure

  datanode:
    image: apache/hadoop:${HADOOPVERSION}
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:${DATADIR}
    env_file:
      - ./hadoop.conf
      - ./.env
    command:
      - /bin/bash
      - -c
      - |
        sleep 40
        mkdir -p ${DATADIR}
        sudo chown hadoop ${DATADIR}
        if [ ! -d $DATADIR ]; then
          echo "Datanode data directory not found: $DATADIR"
          exit 2
        fi
        hdfs --config $HADOOPCONFDIR datanode

  resourcemanager:
    image: apache/hadoop:${HADOOPVERSION}
    container_name: resourcemanager
    restart: always
    env_file:
      - ./hadoop.conf
      - ./.env
    ports:
      - 8088:8088
    command: 
      - /bin/bash
      - -c
      - |
        sleep 50
        /opt/hadoop/bin/yarn --config /opt/hadoop/etc/hadoop resourcemanager

  nodemanager:
    image: apache/hadoop:${HADOOPVERSION}
    container_name: nodemanager
    restart: always
    env_file:
      - ./hadoop.conf
      - ./.env
    command:
      - /bin/bash
      - -c
      - |
        sleep 50
        /opt/hadoop/bin/yarn --config /opt/hadoop/etc/hadoop nodemanager

  historyserver:
    image: apache/hadoop:${HADOOPVERSION}
    container_name: historyserver
    restart: always
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.conf
      - ./.env
    ports:
      - ${YARNHISTSERVERPORT}:${YARNHISTSERVERPORT}
    command:
      - /bin/bash
      - -c
      - |
        sleep 50
        mkdir -p /hadoop/yarn/timeline
        sudo chown hadoop /hadoop/yarn/timeline
        /opt/hadoop/bin/yarn --config $HADOOPCONFDIR historyserver

  spark-master:
    image: apache/spark:${SPARKVERSION}
    container_name: spark-master
    environment:
      - SPARK_NO_DAEMONIZE=true
    env_file:
      - ./.env
      - ./hadoop.conf
    volumes:
      - ./dataplatform/conf_files/spark_conf/:${SPARKCONFDIR}/
    ports:
      - ${SPARKMASTERPORT}:${SPARKMASTERPORT}
    depends_on:
      - namenode1
      - namenode2
    command:
      - /bin/bash
      - -c
      - |
        sleep 50
        /opt/spark/sbin/start-master.sh


  spark-worker:
    image: apache/spark:${SPARKVERSION}
    container_name: spark-worker
    environment:
      - SPARK_NO_DAEMONIZE=true
    env_file:
      - ./.env
      - ./hadoop.conf
    volumes:
      - ./dataplatform/conf_files/spark_conf/:${SPARKCONFDIR}/
    restart: always
    depends_on:
      - spark-master
    command:
      - /bin/bash
      - -c
      - |
        sleep 50
        /opt/spark/sbin/start-worker.sh spark://${SPARKMASTERHOST}:${SPARKMASTERPORT}

  spark-history-server:
    image: apache/spark:${SPARKVERSION}
    container_name: spark-history-server
    environment:
      - SPARK_NO_DAEMONIZE=true
    env_file:
      - ./.env
      - ./hadoop.conf
    volumes:
      - ./dataplatform/conf_files/spark_conf/:${SPARKCONFDIR}/
    ports:
      - ${SPARKHISTSERVERPORT}:${SPARKHISTSERVERPORT}
    restart: always
    command:
      - /bin/bash
      - -c
      - |
        sleep 50
        mkdir -p tmp/spark-events/
        sudo chown spark tmp/spark-events/
        if [ ! -d tmp/spark-events/ ]; then
          echo "Spark logs data directory not found"
          exit 2
        fi
        /opt/spark/sbin/start-history-server.sh
    depends_on:
      - spark-master
      - namenode1
      - namenode2

  spark-yarn-test-env:
    image: apache/spark:${SPARKVERSION}
    container_name: spark-yarn-test-env
    command: "bash"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - HADOOPCONFDIR=/opt/spark/conf
    env_file:
      - ./.env
      - ./hadoop.conf
    volumes:
      - ./dataplatform/conf_files/spark_conf/:${SPARKCONFDIR}/
      - ./dataplatform/conf_files/hadoop_conf/core-site.xml:${SPARKCONFDIR}/core-site.xml
      - ./dataplatform/conf_files/hadoop_conf/hdfs-site.xml:${SPARKCONFDIR}/hdfs-site.xml
      - ./dataplatform/conf_files/hadoop_conf/yarn-site.xml:${SPARKCONFDIR}/yarn-site.xml
      - ./dataplatform/tests:/opt/spark/tests
    stdin_open: true 
    tty: true  

volumes:
  hadoop_namenode1:
  hadoop_namenode2:
  hadoop_datanode:
  hadoop_journal1:
  hadoop_journal2:
  hadoop_journal3:
  hadoop_historyserver:
