<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>01-hdfs</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2//css/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2//css/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2//css/theme/white.css" id="theme">
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2//css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2//css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2//lib/js/html5shiv.js"></script>
  <![endif]-->
  <style>
  .slides {
      font-size: 0.5em;
  }

  .reveal ul {
      display: block;
  }

  .reveal ol {
      display: block;
  }

  strong { 
      color: #E4572E;
  }

  em { 
      color: #3944BC;
  }

  img {
      max-height: 350px !important;
  }

  p {
      text-align: left;
  }

  figcaption {
      font-size: 0.6em !important;
      font-style: italic !important;
  }

  .title {
      color: #29335C !important;
  }

  .h1 {
      color: #29335C !important;
  }

  .h2 {
      color: #29335C !important;
  }

  .subtitle {
      font-style: italic !important;
      text-align: center !important;
      color: #669BBC !important;
  }

  .author {
      text-align: center !important;
  }

  .date {
      font-size: 0.75em !important;
      text-align: center !important;
  }
  </style>
</head>
<body>
  <div class="reveal">
    <div class="slides">


<section class="slide level2">

<p>Services 1. Hadoop HDFS, version: 3.3.6</p>
<ol start="2" type="1">
<li><p>Hadoop YARN, version: 3.3.6</p></li>
<li><p>Spark, version: 3</p></li>
</ol>
</section>
<section>
<section id="hadoop" class="title-slide slide level1">
<h1><strong>Hadoop</strong></h1>

</section>
<section id="hdfs" class="slide level2">
<h2><strong>HDFS</strong></h2>
<figure>
<img data-src="architecture.png" alt="" /><figcaption>Architecture image</figcaption>
</figure>
<h3 id="mode"><strong>Mode</strong></h3>
<p><strong>High-Availability</strong>: two or more NameNodes are grouped into a <strong>nameservice</strong> (<em>myhacluster</em>).</p>
<ul>
<li><p>One active namenode per time.</p></li>
<li><p>Only active namenode manages requests.</p></li>
<li><p>Stand-by namenodes stay updated thanks to JournalNodes</p></li>
<li><p>Every action performed by <em>active</em> namenode is logged through journals: the policy is to write on the majority of Journal Nodes.</p></li>
<li><p>Each component data is persisted through a NFS. Each component has its own directory inside NFS.</p></li>
<li><p><strong>High-availability is resolved client-side. Configuration must be shared between NameNodes, JournalNodes and HDFS clients</strong>. Two default ways (we’re using RequestHedgingProvider) of communicating with a nameservice. For this and a basic HDFS-HA setup, take a look at the <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">[official doc]</a></p></li>
</ul>
<h3 id="configuration"><strong>Configuration</strong></h3>
<p>Configuration properties define the cluster setup, how services find and interact with each other and <em>how clients can communicate with the hadoop cluster</em>. Configuration can be passed to HDFS services (using apache/hadoop docker image) in two ways.</p>
<h4 id="configuration-file"><strong>Configuration file</strong></h4>
<p>It’s a simple <em>.conf</em> file which will be parsed into environmental variables in the container and then parsed into hdfs properties. Such file should have the following syntax:</p>
<blockquote>
<p><em>CORE-SITE.XML_hadoop.http.staticuser.user=root</em></p>
</blockquote>
<p>where the “_” and “=” are split characters <u> and thus only one occurrence of each should appear in the property string </u>and the first part refers to the Hadoop property file this property belongs to (core-site.xml), the middle parte refers to the property name (hadoop.http.staticuser.user) and the third part of the split refers to the value of such property (root). Such .conf file needs to be passed as <em>env_file</em> inside the docker stack file.</p>
<h4 id="property-files"><strong>Property files</strong></h4>
<p>HDFS configuration depends on multiple files, most importantly:</p>
<ul>
<li><strong>core-site.xml</strong> Contains properties that help communication with/between hadoop services, e.g. &gt; <em>CORE-SITE.XML_fs.defaultFS=hdfs://myhacluster.</em></li>
</ul>
<p>An example of core-site.xml with a description of each possible variable can be found in the <a href="https://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-common/core-default.xml">[official template]</a></p>
<ul>
<li><p><strong>hdfs-site.xml</strong> Contains the configuration of the HDFS High-Availability cluster, common properties that help communication with/between hadoop services and settings for the HDFS website. An example of hdfs-site.xml with a description of each possible variable can be found in the <a href="https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">[official template]</a></p></li>
<li><p><strong>capacity-scheduler.xml</strong> Contains properties related to YARN and resource allocation. An example of hdfs-site.xml with a description of each possible variable can be found in the <a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">[official doc]</a></p></li>
<li><p><strong>yarn-site.xml</strong> Contains properties related to YARN and resource allocation and the configuration for YARN websites (ResourceManager, NodeManager HistoryServer). An example of hdfs-site.xml with a description of each possible variable can be found in the <a href="https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">[official template]</a><strong>.</strong> Other “minor” property files:</p></li>
<li><p><strong>hadoop-policy.xml</strong></p></li>
<li><p><strong>hadoop-env.sh</strong> Can define variable that will be translated into environmental variables when the hadoop service is started <em>you won’t see these variables on container setup!</em></p></li>
<li><p><strong>hdfs-rbf-site.xml</strong></p></li>
<li><p><strong>httpfs-site.xml</strong></p></li>
<li><p><strong>kms-acls.xml</strong></p></li>
<li><p><strong>kms-site.xml</strong></p></li>
<li><p><strong>log4j.properties</strong> Defines properties about service logs</p></li>
<li><p><strong>mapred-env.sh</strong></p></li>
<li><p><strong>mapred-site.xml</strong></p></li>
</ul>
<p><strong>Property files must be passed to the container and placed inside the $HADOOP_CONF_DIR folder</strong></p>
<h3 id="components"><strong>Components</strong></h3>
<h4 id="journalnode"><strong>JournalNode</strong></h4>
<p>They are a passive component. Must be in an odd number &gt; 3. They are responsible for logging every transaction occured in the HDFS. the <em>active</em> namenode does and for keeping the state of the high-availability cluster always consistent. They NEED the same configuration passed to NameNodes and DataNodes since they will need to communicate between each-other. After setup, they will be contacted by the two NameNodes (once their setup is completed) which will register to the JournalNodes, which will create the NameNode folder to store its logs and start the synching process. On cluster setup, they need to be formatted just as namenodes.</p>
<p>Three main params needed:</p>
<ul>
<li><p>NameNodes address</p></li>
<li><p>Other journals address</p></li>
<li><p>Log directory path</p></li>
</ul>
<p>Periodically, JournalNodes will synch with each-other to grant consistency between their logs. Every time the active namenode performs an action, it logs the operation on <strong>a majority</strong> of JournalNodes. The standby namenode will constantly synch with the journalnodes to stay updated on the HDFS state. Whenever a NameNode switches from <em>stand-by</em> state to <em>active</em> state, it contacts the journal nodes to assure its state is consistent. They don’t expose interfaces to users.</p>
<h4 id="namenode"><strong>NameNode</strong></h4>
<p>Active component. On first cluster setup, it needs to be formatted</p>
<pre><code>hdfs --config ${HADOOP_HOME}/etc/hadoop namenode -format ${CLUSTERNAME}</code></pre>
<p>If the NameNode you’re starting is a stand-by node, it should be formatted and put to stand-by, e.g.</p>
<pre><code>hdfs namenode -bootstrapStandby</code></pre>
<p>After formatting (if needed), namenodes must be started.</p>
<pre><code>hdfs --config $HADOOPCONFDIR namenode</code></pre>
<p>it registers itself to the JournalNodes. Namenodes should be started after journal nodes are up and running. Namenodes expose three interfaces:</p>
<ul>
<li><p><strong>rpc-address</strong> Contains the address to which clients should send RPCs. It can be defined in hdfs-site.xml. e.g. &gt; <em>dfs.namenode.rpc-address.myhacluster.nn2=namenode2:8020</em></p></li>
<li><p><strong>service-rpc-address</strong> Contains the address to which services should send RPCs. It can be defined in hdfs-site.xml. e.g. &gt; <em>dfs.namenode.servicerpc-address.myhacluster.nn2=namenode2:8021</em></p></li>
<li><p><strong>http-address</strong> Contains the address in which the namenode will expose its web UI. It can be defined in hdfs-site.xml. e.g. &gt; <em>dfs.namenode.http-address.myhacluster.nn2=namenode2:9870</em></p></li>
</ul>
<p>These addresses are used for client purposes as much as setup options, so in case of multiple coexisting networks (as in Docker Swarm) it is mandatory that namenodes bind on address 0.0.0.0 in order to be able to fetch communication from each network. This can be done by setting the following properties: &gt; <em>dfs.namenode.http-address.myhacluster.nn1=0.0.0.0:9870</em></p>
<blockquote>
<p><em>dfs.namenode.servicerpc-bind-host0.0.0.0</em></p>
</blockquote>
<blockquote>
<p><em>dfs.namenode.rpc-bind-host:0.0.0.0</em></p>
</blockquote>
<p>Once both NameNodes have started, one of them must be elected <strong>active</strong> (if there isn’t already an active namenode defined)<strong>.</strong> Namenodes election and eventual failovers can be implemented two ways:</p>
<ul>
<li><p><strong>Manually</strong>: default mode, manually switch the active namenode, e.g.</p>
<pre><code>hdfs haadmin -transitionToActive {namenode_id}</code></pre></li>
<li><p><strong>Automatically</strong>: requires a ZooKeeper quorum. To enable automatic election and failover control of Namenodes, the following property needs to be set to true in <em>hdfs-site.xml</em> &gt;dfs.ha.automatic-failover.enabled=true</p>
<p>Once setting up a ZooKeeper cluster with an odd n &gt; 3 servers, Namenodes can be configured to identify such quorum via a <em>core-site.xml</em> property &gt; ha.zookeeper.quorum={zookeeper1_address}:2181,{zookeeper2_address}:2181,{zookeeper3_address}:2181</p>
<p>If automatic failover is enabled then before starting up, one of the two namenodes must create a znode in the ZK quorum via</p>
<pre><code>hdfs zkfc -formatZK -nonInteractive</code></pre>
<p>Upon creation of the znode and once again before startin the namenode process, a ZooKeeper Failover Controller daemon must be started on all the machine hosting namenodes via</p>
<pre><code>hdfs --daemon start zkfc</code></pre>
<p>Each of the ZKFC will then initiate a session with the ZK quorum and try to get a lock on the previously created znode. The one namenode that successfully acquires the lock becomes the active namenode. The ZKFC on the other namenodes will then inform its namenode to go into standby mode. Periodically (5s by default), the ZKFC sends a ping to its namenode as a health check and in case of <em>one</em> missing health check they proceed to close the previouvsly initiated session with the ZK quorum, initiating a failover.</p></li>
</ul>
<h4 id="datanode"><strong>DataNode</strong></h4>
<p>Doesn’t need much setup if not for the same configuration passed to NameNodes and Journal Nodes. On setup, DataNodes <strong>actively</strong> contact the active NameNode and register themselves to it. It’s not the other way around, namenodes don’t need to know apriori who and where the data nodes are, they will know where they are once they register to them. They send heartbeats and block location information updates on both the active and the stand-by namenode.</p>
<h3 id="communication"><strong>Communication</strong></h3>
<p>Each service endpoints are defined and exposed through property files. <em>In Hadoop, each entity is also an HDFS client</em> which leverages property files to know where to find other services. <u>The resolution of which namenode is the active happens <strong>client-side</strong>. This means that if you want to add a container to the cluster and want to be able to interact with HDFS, not only should it have HDFS installed in it but it should contain all the property files definint the the HA cluster’s configuration.</u></p>
<p>There are two ways of determining who is the active NameNode, and the preferred way can be specified inside hdfs-site.xml by specifying one of the two default methods:</p>
<ul>
<li><p><strong>ConfiguredFailoverProxyProvider</strong>: The active namenode is determined by the ZKFC.</p></li>
<li><p><strong>RequestHedgingProxyProvider</strong>: for the first call, concurrently invokes all namenodes to determine the active one, and on subsequent requests, invokes the active namenode until a fail-over happens.</p></li>
</ul>
<h3 id="setup-requirements"><strong>Setup Requirements</strong></h3>
<ul>
<li>On first-cluster-startup, there’s a strict order to follow: JournalNodes -&gt; Active NN -&gt; Passive NN.</li>
</ul>
</section>
<section id="yarn" class="slide level2">
<h2>YARN</h2>
<p><img data-src="yarn.svg" alt="Architecture image" /> It comes from the same Docker Hub Image as HDFS, so in terms of configuration the same two modalities exists:</p>
<ul>
<li><strong>Configuration files</strong></li>
<li><strong>Property files</strong></li>
</ul>
<p>In terms of property files, YARN relies on the same file HDFS does, meaning that property files should be shared between these two and consistency granted. Specifically, <em>yarn-site.xml</em> allows to define network configurations and services’ addresses. Needless to say, each YARN service must share the same configuration.</p>
<h3 id="resourcemanager">ResourceManager</h3>
<p>Master of the YARN cluster. It offers a few interfaces, each to be configured in <em>yarn-site.xml</em>. &gt; <em>yarn.resourcemanager.address=resourcemanager:8032</em> &gt; <em>yarn.resourcemanager.resource-tracker.address=resourcemanager:8031</em> &gt; <em>yarn.resourcemanager.scheduler.address=resourcemanager:8030</em></p>
<p>As for namenodes and, on a wider scale, each service that needs to be contacted by others, ResourceManager <u>must</u> bind on 0.0.0.0 in case of multiple networks coexisting &gt; <em>yarn.resourcemanager.bind-host=0.0.0.0</em> ### NodeManager Upon creation it registers to the ResourceManager. ### HistoryServer Logs data into HDFS. It is accessed by both Resource Manager and Node Managers. YARN history server must be enabled via <em>yarn-site.xml</em> property &gt; <em>yarn.timeline-service.enabled=true</em> &gt;<br />
and its address specified via &gt; <em>yarn.timeline-service.webapp.address</em></p>
<p>Once again, HistoryServer needs to bind on &gt; <em>yarn.timeline-service.bind-host=0.0.0.0</em></p>
</section></section>
<section id="spark" class="title-slide slide level1">
<h1>SPARK</h1>

</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2//js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2//lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2//plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2//plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2//plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
