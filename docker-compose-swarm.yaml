version: "3.9"
services:
  namenode1:
    image: apache/hadoop:${HADOOPVERSION}
    restart: always
    ports:
      - target: ${NAMENODEPORT}
        published: ${NAMENODEPORT}
    volumes:
      - hadoop_namenode1:${NAMEDIR}
    env_file:
      - ./hadoop.conf
      - ./.env
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p /hadoop/dfs/name
        sudo chown hadoop /hadoop/dfs/name
        if [ ! -d $NAMEDIR ]; then
          echo "Namenode name directory not found: $NAMEDIR"
          exit 2
        fi

        if [ -z "$CLUSTERNAME" ]; then
          echo "Cluster name not specified"
          export CLUSTERNAME=test
        fi

        if [ "`ls -A $NAMEDIR`" == "" ]; then
          echo "Formatting namenode name directory: $NAMEDIR"
          /opt/hadoop/bin/hdfs --config $HADOOPCONFDIR namenode -format $CLUSTERNAME
        else
          echo "$NAMEDIR exists!"
        fi
        hdfs --config $HADOOPCONFDIR namenode
    deploy:
      placement:
        constraints:
          - node.role == master

  namenode2:
    image: apache/hadoop:${HADOOPVERSION}
    restart: always
    ports:
      - target: ${NAMENODE2PORT}
        published: ${NAMENODE2PORT}
    volumes:
      - hadoop_namenode2:${NAMEDIR}
    env_file:
      - ./hadoop.conf
      - ./.env
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p $NAMEDIR
        sudo chown hadoop $NAMEDIR
        if [ "`ls -A $NAMEDIR`" == "" ]; then
          echo "Bootstrappppppping namenode2"
          /opt/hadoop/bin/hdfs namenode -bootstrapStandby
        fi
        hdfs --config $HADOOPCONFDIR namenode
    deploy:
      placement:
        constraints:
          - node.role == master

  journal1:
    image: apache/hadoop:${HADOOPVERSION}
    env_file:
      - ./hadoop.conf
      - ./.env
    volumes:
      - hadoop_journal1:$JOURNALDIR
    command: 
      - /bin/bash
      - -c
      - |
        mkdir -p $JOURNALDIR
        sudo chown hadoop $JOURNALDIR
        hdfs journalnode
    deploy:
      placement:
        constraints:
          - node.role == master

  journal2:
    image: apache/hadoop:${HADOOPVERSION}
    env_file:
      - ./hadoop.conf
      - ./.env
    volumes:
      - hadoop_journal2:$JOURNALDIR
    command:       
      - /bin/bash
      - -c
      - |
        mkdir -p $JOURNALDIR
        sudo chown hadoop $JOURNALDIR
        hdfs journalnode
    deploy:
      placement:
        constraints:
          - node.role == master

  journal3:
    image: apache/hadoop:${HADOOPVERSION}
    env_file:
      - ./hadoop.conf
      - ./.env
    volumes:
      - hadoop_journal3:$JOURNALDIR
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p $JOURNALDIR
        sudo chown hadoop $JOURNALDIR
        hdfs journalnode
    deploy:
      placement:
        constraints:
          - node.role == master

  activator:
    image: apache/hadoop:${HADOOPVERSION}
    env_file:
      - ./hadoop.conf
      - ./.env
    command:
      - /bin/bash
      - -c
      - |
        sleep 30
        hdfs haadmin -transitionToActive nn1
    restart: on-failure

  datanode:
    image: apache/hadoop:${HADOOPVERSION}
    restart: always
    volumes:
      - hadoop_datanode:${DATADIR}
    env_file:
      - ./hadoop.conf
      - ./.env
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p ${DATADIR}
        sudo chown hadoop ${DATADIR}
        if [ ! -d $DATADIR ]; then
          echo "Datanode data directory not found: $DATADIR"
          exit 2
        fi
        hdfs --config $HADOOPCONFDIR datanode
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.role == worker

  resourcemanager:
    image: apache/hadoop:${HADOOPVERSION}
    restart: always
    env_file:
      - ./hadoop.conf
      - ./.env
    ports:
      - target: ${RESOURCEMANAGERPORT}
        published: ${RESOURCEMANAGERPORT}
    command: 
      - /bin/bash
      - -c
      - |
        /opt/hadoop/bin/yarn --config /opt/hadoop/etc/hadoop resourcemanager
    deploy:
      placement:
        constraints:
          - node.role == master

  nodemanager:
    image: apache/hadoop:${HADOOPVERSION}
    restart: always
    env_file:
      - ./hadoop.conf
      - ./.env
    command:
      - /bin/bash
      - -c
      - |
        /opt/hadoop/bin/yarn --config /opt/hadoop/etc/hadoop nodemanager
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.role == worker

  historyserver:
    image: apache/hadoop:${HADOOPVERSION}
    restart: always
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.conf
      - ./.env
    ports:
      - target: ${YARNHISTSERVERPORT}
        published: :${YARNHISTSERVERPORT}
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p /hadoop/yarn/timeline
        sudo chown hadoop /hadoop/yarn/timeline
        /opt/hadoop/bin/yarn --config $HADOOPCONFDIR historyserver
    deploy:
      placement:
        constraints:
          - node.role == worker

  spark-master:
    image: apache/spark:${SPARKVERSION}
    container_name: spark-master
    environment:
      - SPARK_NO_DAEMONIZE=true
    env_file:
      - ./.env
      - ./hadoop.conf
    volumes:
      - ./dataplatform/conf_files/spark_conf/:${SPARKCONFDIR}/
    ports:
      - target: ${SPARKMASTERPORT}
        published: ${SPARKMASTERPORT}
    command:
      - /bin/bash
      - -c
      - |
        /opt/spark/sbin/start-master.sh
    deploy:
      placement:
        constraints:
          - node.role == master

  spark-worker:
    image: apache/spark:${SPARKVERSION}
    environment:
      - SPARK_NO_DAEMONIZE=true
    env_file:
      - ./.env
      - ./hadoop.conf
    volumes:
      - ./dataplatform/conf_files/spark_conf/:${SPARKCONFDIR}/
    restart: always
    command:
      - /bin/bash
      - -c
      - |
        /opt/spark/sbin/start-worker.sh spark://${SPARKMASTERHOST}:${SPARKMASTERPORT}
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.role == worker

  spark-history-server:
    image: apache/spark:${SPARKVERSION}
    environment:
      - SPARK_NO_DAEMONIZE=true
    env_file:
      - ./.env
      - ./hadoop.conf
    volumes:
      - ./dataplatform/conf_files/spark_conf/:${SPARKCONFDIR}/
    ports:
      - target: ${SPARKHISTSERVERPORT}
        published: ${SPARKHISTSERVERPORT}
    restart: always
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p tmp/spark-events/
        sudo chown spark tmp/spark-events/
        if [ ! -d tmp/spark-events/ ]; then
          echo "Spark logs data directory not found"
          exit 2
        fi
        /opt/spark/sbin/start-history-server.sh
    deploy:
      placement:
        constraints:
          - node.role == worker

  spark-yarn-test-env:
    image: apache/spark:${SPARKVERSION}
    command: "bash"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - HADOOPCONFDIR=/opt/spark/conf
    env_file:
      - ./.env
      - ./hadoop.conf
    volumes:
      - ./dataplatform/conf_files/spark_conf/:${SPARKCONFDIR}/
      - ./dataplatform/conf_files/hadoop_conf/core-site.xml:${SPARKCONFDIR}/core-site.xml
      - ./dataplatform/conf_files/hadoop_conf/hdfs-site.xml:${SPARKCONFDIR}/hdfs-site.xml
      - ./dataplatform/conf_files/hadoop_conf/yarn-site.xml:${SPARKCONFDIR}/yarn-site.xml
      - ./dataplatform/tests:/opt/spark/tests
    stdin_open: true 
    tty: true
    deploy:
      placement:
        constraints:
          - node.role == worker  
  portainer:
    init: true
    image: portainer/portainer-ce:latest
    ports:
      - target: 9443
        published: 9443
    volumes:
        - ./mounts/portainer:/data
        - /var/run/docker.sock:/var/run/docker.sock
    restart: always

volumes:
  hadoop_namenode1:
  hadoop_namenode2:
  hadoop_datanode:
  hadoop_journal1:
  hadoop_journal2:
  hadoop_journal3:
  hadoop_historyserver: