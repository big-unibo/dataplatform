#CORE-SITE.XML_fs.defaultFS=hdfs://namenode:9000
CORE-SITE.XML_hadoop.http.staticuser.user=root
CORE-SITE.XML_hadoop.proxyuser.hue.hosts=*
CORE-SITE.XML_hadoop.proxyuser.hue.groups=*
CORE-SITE.XML_io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec
HDFS-SITE.XML_dfs.webhdfs.enabled=true
HDFS-SITE.XML_dfs.permissions.enabled=false
HDFS-SITE.XML_dfs.namenode.datanode.registration.ip-hostname-check=false
HDFS-SITE.XML_dfs.replication=3
HDFS-SITE.XML_dfs.namenode.name.dir=file:///hadoop/dfs/name
HDFS-SITE.XML_dfs.datanode.data.dir=file:///hadoop/dfs/data


###### HDFS HA conf ########
CORE-SITE.XML_fs.defaultFS=hdfs://myhacluster
#Declares an entity made of namenodes
HDFS-SITE.XML_dfs.nameservices=myhacluster
#Defines canonical names for namenodes in sh entity
HDFS-SITE.XML_dfs.ha.namenodes.myhacluster=nn1,nn2
#Define RPC addresses for such namenodes

HDFS-SITE.XML_dfs.namenode.rpc-address.myhacluster.nn1=namenode1:8020
HDFS-SITE.XML_dfs.namenode.rpc-address.myhacluster.nn2=namenode2:8020

HDFS-SITE.XML_dfs.namenode.http-address.myhacluster.nn1=namenode1:9870
HDFS-SITE.XML_dfs.namenode.http-address.myhacluster.nn2=namenode2:9871


#Strategy to ensure that both NN can't be active: rn, just does nothing
HDFS-SITE.XML_dfs.ha.fencing.methods=shell(/bin/true)
#How to manage the two namenodes, two classes possibile
HDFS-SITE.XML_dfs.client.failover.proxy.provider.myhacluster=org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider 
#Automatically recover from namenode failures, need Zookeper
HDFS-SITE.XML_dfs.ha.automatic-failover.enabled=false
HDFS-SITE.XML_dfs.namenode.shared.edits.dir=qjournal://journal1:8485;journal2:8485;journal3:8485/myhacluster
HDFS-SITE.XML_dfs.journalnode.edits.dir=/data/journalnode

LOG4J.PROPERTIES_log4j.rootLogger=INFO, stdout
LOG4J.PROPERTIES_log4j.appender.stdout=org.apache.log4j.ConsoleAppender
LOG4J.PROPERTIES_log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
LOG4J.PROPERTIES_log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n

#MAPRED-SITE.XML_mapreduce.framework.name=yarn
#MAPRED-SITE.XML_yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop
#MAPRED-SITE.XML_mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop
#MAPRED-SITE.XML_mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop
MAPRED-SITE.XML_mapreduce.jobhistory.webapp.address=historyserver:19888

YARN-SITE.XML_yarn.nodemanager.pmem-check-enabled=false
YARN-SITE.XML_yarn.nodemanager.delete.debug-delay-sec=600
YARN-SITE.XML_yarn.nodemanager.vmem-check-enabled=false
YARN-SITE.XML_yarn.nodemanager.aux-services=mapreduce_shuffle
YARN-SITE.XML_yarn.scheduler.maximum-allocation-mb=10000
YARN-SITE.XML_yarn.nodemanager.resource.memory-mb=10000
YARN-SITE.XML_yarn.log-aggregation-enable=true
YARN-SITE.XML_yarn.log.server.url=http://historyserver:8188/hadoop/yarn/timeline

YARN-SITE.XML_yarn.resourcemanager.hostname=resourcemanager
YARN-SITE.XML_yarn.resourcemanager.address=resourcemanager:8032
YARN-SITE.XML_yarn.resourcemanager.scheduler.address=resourcemanager:8030
YARN-SITE.XML_yarn.resourcemanager.resource-tracker.address=resourcemanager:8031
YARN-SITE.XML_yarn.resourcemanager.system-metrics-publisher.enabled=true

# YARN timeline service
YARN-SITE.XML_yarn.timeline-service.enabled=true
YARN-SITE.XML_yarn.timeline-service.generic-application-history.enabled=true
YARN-SITE.XML_yarn.timeline-service.leveldb-timeline-store.path=/hadoop/yarn/timeline
YARN-SITE.XML_yarn.timeline-service.webapp.address=historyserver:19888
YARN-SITE.XML_yarn.timeline-service.hostname=historyserver
YARN-SITE.XML_yarn.timeline-service.entity-group-fs-store.summary-entity-types=YARN_APPLICATION,YARN_APPLICATION_ATTEMPT,YARN_CONTAINER,spark_event_v01

CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-applications=10000
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-am-resource-percent=0.1
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.resource-calculator=org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.queues=default
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.capacity=100
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.user-limit-factor=1
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.maximum-capacity=100
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.state=RUNNING
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl.submit.applications=*
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl.administer.queue=*
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.node-locality-delay=40
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings=
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings-override.enable=false