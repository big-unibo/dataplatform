version: "3.9"
services:
  namenode1:
    image: apache/hadoop:${HADOOPVERSION}
    ports:
      - ${NAMENODE1PORT}:${NAMENODE1PORT}
    volumes:
      - namenode1_data:${NAMEDIR}
      - namenode1_config:${HADOOPCONFDIR}
    command:
      - /bin/bash
      - -c
      - |
        sleep 20
        mkdir -p ${NAMEDIR}
        sudo chown hadoop ${NAMEDIR}
        if [ ! -d ${NAMEDIR} ]; then
          echo "Namenode name directory not found: ${NAMEDIR}"
          exit 2
        fi

        if [ -z "${CLUSTERNAME}" ]; then
          echo "Cluster name not specified"
          export CLUSTERNAME=${CLUSTERNAME}
        fi

        if [ "`ls -A ${NAMEDIR}`" == "" ]; then
          echo "Formatting namenode name directory: ${NAMEDIR}"
          ${HADOOPHOME}hdfs --config ${HADOOPCONFDIR} namenode -format -y ${CLUSTERNAME}
        else
          echo "${NAMEDIR} exists!"
        fi
        hdfs zkfc -formatZK -nonInteractive
        echo "Done formatting ZK ... starting zkfc"
        hdfs --daemon start zkfc > zkfc_logs.txt
        echo "Started zkfc"
        hdfs --config ${HADOOPCONFDIR} namenode
    deploy:
      placement:
        constraints:
          - node.role == manager
          - node.hostname != CB-Mass-Node1
          - node.labels.running_namenode2==0
    networks:
      - BIG-dataplatform-network 
    
  namenode2:
    image: apache/hadoop:${HADOOPVERSION}
    ports:
      - ${NAMENODE2PORT}:${NAMENODE2PORT}
    volumes:
      - namenode2_data:${NAMEDIR}
      - namenode2_config:${HADOOPCONFDIR}
    command:
      - /bin/bash
      - -c
      - |
        sleep 30
        mkdir -p ${NAMEDIR}
        sudo chown hadoop ${NAMEDIR}
        if [ "`ls -A ${NAMEDIR}`" == "" ]; then
          echo "Bootstrappppppping namenode2"
          ${HADOOPHOME}hdfs namenode -bootstrapStandby
        fi
        echo "starting zkfc"
        hdfs --daemon start zkfc > zkfc_logs.txt
        echo "Started zkfc"
        hdfs --config ${HADOOPCONFDIR} namenode
    deploy:
      placement:
        constraints:
          - node.role == manager
          - node.hostname != CB-Mass-Node1
          - node.labels.running_namenode1==0
    networks:
      - BIG-dataplatform-network 

  journal1:
    image: apache/hadoop:${HADOOPVERSION}
    volumes:
      - journal1_data:${JOURNALDIR}
      - hadoop_config:${HADOOPCONFDIR}
    command: 
      - /bin/bash
      - -c
      - |
        mkdir -p ${JOURNALDIR}
        sudo chown hadoop ${JOURNALDIR}
        hdfs journalnode
    deploy:
      placement:
        constraints:
          - node.role == manager
          - node.hostname != CB-Mass-Node1
          - node.labels.running_journal2==0
          - node.labels.running_journal3==0
    networks:
      - BIG-dataplatform-network

  journal2:
    image: apache/hadoop:${HADOOPVERSION}
    volumes:
      - journal2_data:${JOURNALDIR}
      - hadoop_config:${HADOOPCONFDIR}
    command:       
      - /bin/bash
      - -c
      - |
        mkdir -p ${JOURNALDIR}
        sudo chown hadoop ${JOURNALDIR}
        hdfs journalnode
    deploy:
      placement:
        constraints:
          - node.role == manager
          - node.hostname != CB-Mass-Node1
          - node.labels.running_journal1==0
          - node.labels.running_journal3==0
    networks:
      - BIG-dataplatform-network 

  journal3:
    image: apache/hadoop:${HADOOPVERSION}
    volumes:
      - journal3_data:${JOURNALDIR}
      - hadoop_config:${HADOOPCONFDIR}
    command:
      - /bin/bash
      - -c
      - |
        mkdir -p ${JOURNALDIR}
        sudo chown hadoop ${JOURNALDIR}
        hdfs journalnode
    deploy:
      placement:
        constraints:
          - node.role == manager
          - node.hostname != CB-Mass-Node1
          - node.labels.running_journal2==0
          - node.labels.running_journal1==0
    networks:
      - BIG-dataplatform-network

  datanode1:
    image: apache/hadoop:${HADOOPVERSION}
    volumes:
      - hadoop_config:${HADOOPCONFDIR}
      - ${DATANODEDISK1}:${DATANODEDIR1}
      - ${DATANODEDISK2}:${DATANODEDIR2}
      - ${DATANODEDISK3}:${DATANODEDIR3}
    command:
      - /bin/bash
      - -c
      - |
        sleep 30
        sudo mkdir -p ${DATADIR}
        sudo chown hadoop ${DATADIR}
        sudo chown hadoop ${DATANODEDIR1}
        sudo chown hadoop ${DATANODEDIR2}
        sudo chown hadoop ${DATANODEDIR3}
        if [ ! -d ${DATADIR} ]; then
          echo "Datanode data directory not found: ${DATADIR}"
          #exit 2
        fi
        hdfs --config ${HADOOPCONFDIR} datanode
    deploy:
      placement:
        constraints:
          - node.role == worker
          - node.hostname != CB-Mass-Node1
          - node.hostname != isi-bigcluster-pnnr-11
          - node.hostname != isi-bigcluster-pnnr-12
          - node.labels.running_datanode2==0
          - node.labels.running_datanode3==0
          - node.labels.running_datanode4==0
          - node.labels.running_datanode5==0
          - node.labels.running_datanode6==0
          - node.labels.running_datanode7==0
          - node.labels.running_namenode1==0
          - node.labels.running_namenode2==0
    networks:
      - BIG-dataplatform-network

  datanode2:
    image: apache/hadoop:${HADOOPVERSION}
    volumes:
      - hadoop_config:${HADOOPCONFDIR}
      - ${DATANODEDISK1}:${DATANODEDIR1}
      - ${DATANODEDISK2}:${DATANODEDIR2}
      - ${DATANODEDISK3}:${DATANODEDIR3}
    command:
      - /bin/bash
      - -c
      - |
        sleep 30
        sudo mkdir -p ${DATADIR}
        sudo chown hadoop ${DATADIR}
        sudo chown hadoop ${DATANODEDIR1}
        sudo chown hadoop ${DATANODEDIR2}
        sudo chown hadoop ${DATANODEDIR3}

        if [ ! -d ${DATADIR} ]; then
          echo "Datanode data directory not found: ${DATADIR}"
          #exit 2
        fi
        hdfs --config ${HADOOPCONFDIR} datanode
    deploy:
      placement:
        constraints:
          - node.role == worker
          - node.hostname != CB-Mass-Node1
          - node.hostname != isi-bigcluster-pnnr-11
          - node.hostname != isi-bigcluster-pnnr-12
          - node.labels.running_datanode1==0
          - node.labels.running_datanode3==0
          - node.labels.running_datanode4==0
          - node.labels.running_datanode5==0
          - node.labels.running_datanode6==0
          - node.labels.running_datanode7==0
          - node.labels.running_namenode1==0
          - node.labels.running_namenode2==0
    networks:
      - BIG-dataplatform-network

  datanode3:
    image: apache/hadoop:${HADOOPVERSION}
    volumes:
      - hadoop_config:${HADOOPCONFDIR}
      - ${DATANODEDISK1}:${DATANODEDIR1}
      - ${DATANODEDISK2}:${DATANODEDIR2}
      - ${DATANODEDISK3}:${DATANODEDIR3}
    command:
      - /bin/bash
      - -c
      - |
        sleep 30
        sudo mkdir -p ${DATADIR}
        sudo chown hadoop ${DATADIR}
        sudo chown hadoop ${DATANODEDIR1}
        sudo chown hadoop ${DATANODEDIR2}
        sudo chown hadoop ${DATANODEDIR3}

        if [ ! -d ${DATADIR} ]; then
          echo "Datanode data directory not found: ${DATADIR}"
          #exit 2
        fi
        hdfs --config ${HADOOPCONFDIR} datanode
    deploy:
      placement:
        constraints:
          - node.role == worker
          - node.labels.running_datanode2==0
          - node.labels.running_datanode1==0
          - node.labels.running_datanode4==0
          - node.labels.running_datanode5==0
          - node.labels.running_datanode6==0
          - node.labels.running_datanode7==0
          - node.labels.running_namenode1==0
          - node.labels.running_namenode2==0
          - node.hostname != CB-Mass-Node1
          - node.hostname != isi-bigcluster-pnnr-11
          - node.hostname != isi-bigcluster-pnnr-12

    networks:
      - BIG-dataplatform-network

  datanode4:
    image: apache/hadoop:${HADOOPVERSION}
    volumes:
      - hadoop_config:${HADOOPCONFDIR}
      - ${DATANODEDISK1}:${DATANODEDIR1}
      - ${DATANODEDISK2}:${DATANODEDIR2}
      - ${DATANODEDISK3}:${DATANODEDIR3}
    command:
      - /bin/bash
      - -c
      - |
        sleep 30
        sudo mkdir -p ${DATADIR}
        sudo chown hadoop ${DATADIR}
        sudo chown hadoop ${DATANODEDIR1}
        sudo chown hadoop ${DATANODEDIR2}
        sudo chown hadoop ${DATANODEDIR3}
        if [ ! -d ${DATADIR} ]; then
          echo "Datanode data directory not found: ${DATADIR}"
          #exit 2
        fi
        hdfs --config ${HADOOPCONFDIR} datanode
    deploy:
      placement:
        constraints:
          - node.role == worker
          - node.labels.running_datanode2==0
          - node.labels.running_datanode3==0
          - node.labels.running_datanode1==0
          - node.labels.running_datanode5==0
          - node.labels.running_datanode6==0
          - node.labels.running_datanode7==0
          - node.labels.running_namenode1==0
          - node.labels.running_namenode2==0
          - node.hostname != CB-Mass-Node1
          - node.hostname != isi-bigcluster-pnnr-11
          - node.hostname != isi-bigcluster-pnnr-12
    networks:
      - BIG-dataplatform-network

  datanode5:
    image: apache/hadoop:${HADOOPVERSION}
    volumes:
      - hadoop_config:${HADOOPCONFDIR}
      - ${DATANODEDISK1}:${DATANODEDIR1}
      - ${DATANODEDISK2}:${DATANODEDIR2}
      - ${DATANODEDISK3}:${DATANODEDIR3}
    command:
      - /bin/bash
      - -c
      - |
        sleep 30
        sudo mkdir -p ${DATADIR}
        sudo chown hadoop ${DATADIR}
        sudo chown hadoop ${DATANODEDIR1}
        sudo chown hadoop ${DATANODEDIR2}
        sudo chown hadoop ${DATANODEDIR3}
        if [ ! -d ${DATADIR} ]; then
          echo "Datanode data directory not found: ${DATADIR}"
          #exit 2
        fi
        hdfs --config ${HADOOPCONFDIR} datanode
    deploy:
      placement:
        constraints:
          - node.role == worker
          - node.labels.running_datanode1==0
          - node.labels.running_datanode2==0
          - node.labels.running_datanode3==0
          - node.labels.running_datanode4==0
          - node.labels.running_datanode6==0
          - node.labels.running_datanode7==0
          - node.labels.running_namenode1==0
          - node.labels.running_namenode2==0
          - node.hostname != CB-Mass-Node1
          - node.hostname != isi-bigcluster-pnnr-11
          - node.hostname != isi-bigcluster-pnnr-12
    networks:
      - BIG-dataplatform-network

  datanode6:
    image: apache/hadoop:${HADOOPVERSION}
    volumes:
      - hadoop_config:${HADOOPCONFDIR}
      - ${DATANODEDISK1}:${DATANODEDIR1}
      - ${DATANODEDISK2}:${DATANODEDIR2}
      - ${DATANODEDISK3}:${DATANODEDIR3}
    command:
      - /bin/bash
      - -c
      - |
        sleep 30
        sudo mkdir -p ${DATADIR}
        sudo chown hadoop ${DATADIR}
        sudo chown hadoop ${DATANODEDIR1}
        sudo chown hadoop ${DATANODEDIR2}
        sudo chown hadoop ${DATANODEDIR3}
        if [ ! -d ${DATADIR} ]; then
          echo "Datanode data directory not found: ${DATADIR}"
          #exit 2
        fi
        hdfs --config ${HADOOPCONFDIR} datanode
    deploy:
      placement:
        constraints:
          - node.role == worker
          - node.labels.running_datanode1==0
          - node.labels.running_datanode2==0
          - node.labels.running_datanode3==0
          - node.labels.running_datanode4==0
          - node.labels.running_datanode5==0
          - node.labels.running_datanode7==0
          - node.labels.running_namenode1==0
          - node.labels.running_namenode2==0
          - node.hostname != CB-Mass-Node1
          - node.hostname != isi-bigcluster-pnnr-11
          - node.hostname != isi-bigcluster-pnnr-12
    networks:
      - BIG-dataplatform-network

  datanode7:
    image: apache/hadoop:${HADOOPVERSION}
    volumes:
      - hadoop_config:${HADOOPCONFDIR}
      - ${DATANODEDISK1}:${DATANODEDIR1}
      - ${DATANODEDISK2}:${DATANODEDIR2}
      - ${DATANODEDISK3}:${DATANODEDIR3}
    command:
      - /bin/bash
      - -c
      - |
        sleep 30
        sudo mkdir -p ${DATADIR}
        sudo chown hadoop ${DATADIR}
        sudo chown hadoop ${DATANODEDIR1}
        sudo chown hadoop ${DATANODEDIR2}
        sudo chown hadoop ${DATANODEDIR3}
        if [ ! -d ${DATADIR} ]; then
          echo "Datanode data directory not found: ${DATADIR}"
          #exit 2
        fi
        hdfs --config ${HADOOPCONFDIR} datanode
    deploy:
      placement:
        constraints:
          - node.role == worker
          - node.labels.running_datanode1==0
          - node.labels.running_datanode2==0
          - node.labels.running_datanode3==0
          - node.labels.running_datanode4==0
          - node.labels.running_datanode5==0
          - node.labels.running_datanode6==0
          - node.labels.running_namenode1==0
          - node.labels.running_namenode2==0
          - node.hostname != CB-Mass-Node1
          - node.hostname != isi-bigcluster-pnnr-11
          - node.hostname != isi-bigcluster-pnnr-12
    networks:
      - BIG-dataplatform-network

  datanode8:
    image: apache/hadoop:${HADOOPVERSION}
    volumes:
      - hadoop_config:${HADOOPCONFDIR}
      - ${DATANODEDISK1}:${DATANODEDIR1}
      - ${DATANODEDISK2}:${DATANODEDIR2}
    command:
      - /bin/bash
      - -c
      - |
        sleep 30
        sudo mkdir -p ${DATADIR}
        sudo chown hadoop ${DATADIR}
        sudo chown hadoop ${DATANODEDIR1}
        sudo chown hadoop ${DATANODEDIR2}
        export HADOOP_OPTS="-Ddfs.data.dir=file://${DATANODEDIR1},file://${DATANODEDIR2}"
        if [ ! -d ${DATADIR} ]; then
          echo "Datanode data directory not found: ${DATADIR}"
          #exit 2
        fi
        hdfs --config ${HADOOPCONFDIR} datanode
    deploy:
      placement:
        constraints:
          - node.role == worker
          - node.labels.running_datanode1==0
          - node.labels.running_datanode2==0
          - node.labels.running_datanode3==0
          - node.labels.running_datanode4==0
          - node.labels.running_datanode5==0
          - node.labels.running_datanode6==0
          - node.labels.running_datanode7==0
          - node.labels.running_namenode1==0
          - node.labels.running_namenode2==0
          - node.labels.number_of_disks==2
          - node.hostname != CB-Mass-Node1
    networks:
      - BIG-dataplatform-network

volumes:
  namenode1_config:
    driver_opts:
      type: nfs
      o: addr=${NFSADDRESS},rw,nfsvers=4
      device: :${NFSPATH}/dataplatform_config/namenode1_conf/
    
  namenode1_data:
    driver_opts:
      type: nfs
      o: addr=${NFSADDRESS},rw,nfsvers=4
      device: :${NFSPATH}/dataplatform_config/namenode1_data/

  namenode2_config:
    driver_opts:
      type: nfs
      o: addr=${NFSADDRESS},rw,nfsvers=4
      device: :${NFSPATH}/dataplatform_config/namenode2_conf/

  namenode2_data:
    driver_opts:
      type: nfs
      o: addr=${NFSADDRESS},rw,nfsvers=4
      device: :${NFSPATH}/dataplatform_config/namenode2_data/

  journal1_data:
    driver_opts:
      type: nfs
      o: addr=${NFSADDRESS},rw,nfsvers=4
      device: :${NFSPATH}/dataplatform_config/journal1_data/

  journal2_data:
    driver_opts:
      type: nfs
      o: addr=${NFSADDRESS},rw,nfsvers=4
      device: :${NFSPATH}/dataplatform_config/journal2_data/

  journal3_data:
    driver_opts:
      type: nfs
      o: addr=${NFSADDRESS},rw,nfsvers=4
      device: :${NFSPATH}/dataplatform_config/journal3_data/

  hadoop_config:
    driver: local
    driver_opts:
      type: nfs
      o: addr=${NFSADDRESS},rw,nfsvers=4,nolock,hard
      device: ":${NFSPATH}/dataplatform_config/hadoop_conf/"
  
networks:
  BIG-dataplatform-network:
    external: true
    name: BIG-dataplatform-network
